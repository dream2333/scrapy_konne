from math import inf
import os
from zoneinfo import ZoneInfo
from aiohttp import ClientSession
from scrapy.crawler import Crawler
from scrapy import signals
from scrapy.utils.log import logger
from tabulate import tabulate
from wcwidth import wcswidth


class KonneWechatBotExtension:

    def __init__(self, crawler: Crawler):
        self.bot_url = crawler.settings.get("WECHAT_BOT_URL")
        self.failure_threshold = crawler.settings.get("WECHAT_BOT_REQ_FAILURE_THRESHOLD", 0)
        self.log_error_threshold = crawler.settings.get("WECHAT_BOT_LOG_ERROR_THRESHOLD", 0)
        self.elapsed_time_threshold = crawler.settings.get("ELAPSED_TIME_THRESHOLD", float("inf"))
        if self.elapsed_time_threshold <= 0 or self.elapsed_time_threshold is None:
            self.elapsed_time_threshold = inf
        self.admin_url = crawler.settings.get("ADMIN_PAGE_URL")
        self.stats_collector = crawler.stats

    @classmethod
    def from_crawler(cls: "KonneWechatBotExtension", crawler: Crawler):
        extension = cls(crawler)
        crawler.signals.connect(extension.spider_closed, signal=signals.spider_closed)
        return extension

    async def spider_closed(self, spider, reason):
        # è·å–æœ¬åœ°æ—¶åŒº
        local_tz = ZoneInfo("Asia/Shanghai")
        # å°† UTC æ—¶é—´è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´
        jobid = os.getenv("SCRAPYD_JOB", "N/A")
        project_name = os.getenv("SCRAPY_PROJECT", "N/A")
        stats = self.stats_collector.get_stats(spider)
        start_time = stats.get("start_time")
        finish_time = stats.get("finish_time")
        elapsed_time_seconds = int(stats.get("elapsed_time_seconds", 0))
        item_scraped_count = stats.get("item_scraped_count", 0)
        error_log = stats.get("log_count/ERROR", 0)
        retry_max_reached = stats.get("retry/max_reached", 0)
        item_dropped_count = stats.get("item_dropped_count", 0)
        total_scraped_count = item_dropped_count + item_scraped_count
        if (
            total_scraped_count == 0
            or retry_max_reached > self.failure_threshold
            or error_log > self.log_error_threshold
            or elapsed_time_seconds > self.elapsed_time_threshold
            or reason != "finished"
        ):
            data = [
                ["æ€»è€—æ—¶", elapsed_time_seconds, self.elapsed_time_threshold],
                ["itemæäº¤", item_scraped_count, "æ— "],
                ["itemè¿‡æ»¤", item_dropped_count, "æ— "],
                ["itemæ€»è®¡", total_scraped_count, 0],
                ["å¤±è´¥è¯·æ±‚", retry_max_reached, self.failure_threshold],
                ["é”™è¯¯æ—¥å¿—", error_log, self.log_error_threshold],
            ]

            # è¡¨å¤´
            headers = ["æŒ‡æ ‡", "ç»“æœ", "é˜ˆå€¼"]
            col_widths = [max(wcswidth(str(x)) for x in col) for col in zip(*data, headers)]
            table = tabulate(data, headers=headers, tablefmt="simple", colalign=("left",) * len(col_widths))
            md_content = f"""
<font color="warning">{spider.name}</font>ç›¸å…³ç»Ÿè®¡æ•°æ®å¼‚å¸¸ï¼Œè¯·ç›¸å…³åŒäº‹æ³¨æ„ã€‚

<font color="comment">{project_name}</font>

<font color="comment">{jobid}</font>

å¼€å§‹: <font color="comment">{start_time.astimezone(local_tz).strftime("%Y-%m-%d %H:%M:%S")}</font>

ç»“æŸ: <font color="comment">{finish_time.astimezone(local_tz).strftime("%Y-%m-%d %H:%M:%S")}</font>

ç»“æŸåŸå› ,  <font color="comment">{reason}</font>


{table}


[ğŸ““ç‚¹å‡»æŸ¥çœ‹æ—¥å¿—]({self.admin_url}/#/logs/{project_name}/{spider.name}/{jobid})
"""
            data = {
                "msgtype": "markdown",
                "markdown": {"content": md_content},
                "mentioned_list": ["@all"],
            }
            logger.info(f"çˆ¬è™«{spider.name}æœ‰å¼‚å¸¸çŠ¶æ€ï¼Œå·²å‘é€åˆ°ä¼ä¸šå¾®ä¿¡")
            async with ClientSession() as session:
                async with session.post(self.bot_url, json=data) as response:
                    if response.status == 200:
                        result = await response.json()
                        if result.get("errcode") == 0:
                            return True
        else:
            logger.info(f"çˆ¬è™«{spider.name}æ­£å¸¸ç»“æŸï¼Œæ— éœ€å‘é€åˆ°ä¼ä¸šå¾®ä¿¡")
