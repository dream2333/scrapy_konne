import asyncio
from math import inf
import os
from zoneinfo import ZoneInfo
from aiohttp import ClientSession
from scrapy.crawler import Crawler
from scrapy import signals
from weakref import WeakKeyDictionary
from scrapy.utils.log import logger
from tabulate import tabulate
from wcwidth import wcswidth


class KonneHttpLogExtension:
    """ç”¨äºå‘å…¬å¸æ¥å£æäº¤æ—¥å¿—"""

    def __init__(self, interval: int):
        self.spiders_stats = WeakKeyDictionary()
        self.interval = interval
        self.session = ClientSession()
        self.log_task = None
        self.loop = asyncio.get_event_loop()

    @classmethod
    def from_crawler(cls: "KonneHttpLogExtension", crawler: Crawler):
        section_log_ip = crawler.settings.get("SECTION_LOG_IP")
        log_interval = crawler.settings.get("UPLOAD_LOG_INTERVAL")
        extension: "KonneHttpLogExtension" = cls(log_interval)
        # æ·»åŠ ä¿¡å·
        crawler.signals.connect(extension.spider_opened, signal=signals.spider_opened)
        crawler.signals.connect(extension.spider_closed, signal=signals.spider_closed)
        crawler.signals.connect(extension.item_scraped, signal=signals.item_passed)
        crawler.signals.connect(extension.request_scheduled, signal=signals.request_scheduled)
        cls.section_log_url = f"http://{section_log_ip}/Log/AddSectionLog"
        return extension

    def spider_opened(self, spider):
        self.spiders_stats[spider] = {
            "LogID": "",
            "SiteID": spider.site_id,
            "TotalCount": 0,
            "AddCount": 0,
            "Message": "scrapy-log",
            "CreateTime": "",
            "ClientID": spider.client_id,
        }
        # å®šæ—¶ä»»åŠ¡ï¼Œ30ç§’æäº¤ä¸€æ¬¡æ—¥å¿—
        self.log_task = self.loop.create_task(self.log_timer(spider))

    async def spider_closed(self, spider, reason):
        # å–æ¶ˆå®šæ—¶ä»»åŠ¡
        self.log_task.cancel()
        logger.info(f"åº·å¥ˆæ—¥å¿—è®°å½•å™¨[{spider.name}]å·²å…³é—­ï¼Œçˆ¬è™«çŠ¶æ€:{reason}")
        await self.log_stats(spider)
        await self.session.close()

    def request_scheduled(self, request, spider):
        # æ¯æ¬¡è¯·æ±‚ï¼ŒstatsåŠ 1
        self.spiders_stats[spider]["TotalCount"] += 1

    def item_scraped(self, item, response, spider):
        # æ¯æ¬¡æˆåŠŸæ”¶å–ä¸€ä¸ªitemï¼ŒstatsåŠ 1
        self.spiders_stats[spider]["AddCount"] += 1

    async def log_timer(self, spider):
        """æ‰“ç‚¹è®¡æ—¶å™¨ï¼Œæ¯éš”intervalç§’æäº¤ä¸€æ¬¡æ—¥å¿—"""
        while True:
            await asyncio.sleep(self.interval)
            await self.log_stats(spider)

    async def log_stats(self, spider):
        """æäº¤æ—¥å¿—å¹¶è¾“å‡º"""
        result = await self.send_log(spider)
        upload_log = self.spiders_stats[spider]
        formatted_stats = f'æ·»åŠ æ•°ï¼š{upload_log["AddCount"]} è¯·æ±‚æ•°ï¼š{upload_log["TotalCount"]}'
        if result:
            logger.info(f"[{upload_log['ClientID']}] æ—¥å¿—æäº¤æˆåŠŸ: {formatted_stats}")
            # æäº¤æˆåŠŸåï¼Œé‡ç½®stats
            upload_log["AddCount"] = 0
            upload_log["TotalCount"] = 0
        else:
            logger.error(f"[{upload_log['ClientID']}] æ—¥å¿—æäº¤å¤±è´¥,å½“å‰çŠ¶æ€ä¸º: {formatted_stats}")

    async def send_log(self, spider):
        """æäº¤æ—¥å¿—"""
        upload_log = self.spiders_stats[spider]
        async with self.session.post(self.section_log_url, data=upload_log) as response:
            if response.status == 200:
                result = await response.json()
                if result.get("code") == 0:
                    return True
        return False


class KonneWechatBotExtension:

    def __init__(self, crawler: Crawler):
        self.bot_url = crawler.settings.get("WECHAT_BOT_URL")
        self.failure_threshold = crawler.settings.get("WECHAT_BOT_REQ_FAILURE_THRESHOLD", 0)
        self.log_error_threshold = crawler.settings.get("WECHAT_BOT_LOG_ERROR_THRESHOLD", 0)
        self.elapsed_time_threshold = crawler.settings.get("ELAPSED_TIME_THRESHOLD", float("inf"))
        if self.elapsed_time_threshold <= 0 or self.elapsed_time_threshold is None:
            self.elapsed_time_threshold = inf
        self.admin_url = crawler.settings.get("ADMIN_PAGE_URL")
        self.stats_collector = crawler.stats

    @classmethod
    def from_crawler(cls: "KonneWechatBotExtension", crawler: Crawler):
        extension: "KonneHttpLogExtension" = cls(crawler)
        crawler.signals.connect(extension.spider_closed, signal=signals.spider_closed)
        return extension

    async def spider_closed(self, spider, reason):
        # è·å–æœ¬åœ°æ—¶åŒº
        local_tz = ZoneInfo("Asia/Shanghai")
        # å°† UTC æ—¶é—´è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´
        jobid = os.getenv("SCRAPYD_JOB", "N/A")
        project_name = os.getenv("SCRAPY_PROJECT", "N/A")
        stats = self.stats_collector.get_stats(spider)
        start_time = stats.get("start_time")
        finish_time = stats.get("finish_time")
        elapsed_time_seconds = int(stats.get("elapsed_time_seconds", 0))
        item_scraped_count = stats.get("item_scraped_count", 0)
        error_log = stats.get("log_count/ERROR", 0)
        retry_max_reached = stats.get("retry/max_reached", 0)
        item_dropped_count = stats.get("item_dropped_count", 0)
        total_scraped_count = item_dropped_count + item_scraped_count
        if (
            total_scraped_count == 0
            or retry_max_reached > self.failure_threshold
            or error_log > self.log_error_threshold
            or elapsed_time_seconds > self.elapsed_time_threshold
        ):
            data = [
                ["æ€»è€—æ—¶", elapsed_time_seconds, self.elapsed_time_threshold],
                ["itemæäº¤", item_scraped_count, "æ— "],
                ["itemè¿‡æ»¤", item_dropped_count, "æ— "],
                ["itemæ€»è®¡", total_scraped_count, 0],
                ["å¤±è´¥è¯·æ±‚", retry_max_reached, self.failure_threshold],
                ["é”™è¯¯æ—¥å¿—", error_log, self.log_error_threshold],
            ]

            # è¡¨å¤´
            headers = ["æŒ‡æ ‡", "ç»“æœ", "é˜ˆå€¼"]
            col_widths = [max(wcswidth(str(x)) for x in col) for col in zip(*data, headers)]
            table = tabulate(data, headers=headers, tablefmt="simple", colalign=("left",) * len(col_widths))
            md_content = f"""
<font color="warning">{spider.name}</font>ç›¸å…³ç»Ÿè®¡æ•°æ®å¼‚å¸¸ï¼Œè¯·ç›¸å…³åŒäº‹æ³¨æ„ã€‚

<font color="comment">{project_name}</font>

<font color="comment">{jobid}</font>

å¼€å§‹: <font color="comment">{start_time.astimezone(local_tz).strftime("%Y-%m-%d %H:%M:%S")}</font>

ç»“æŸ: <font color="comment">{finish_time.astimezone(local_tz).strftime("%Y-%m-%d %H:%M:%S")}</font>

ç»“æŸåŸå› ,  <font color="comment">{reason}</font>


{table}


[ğŸ““ç‚¹å‡»æŸ¥çœ‹æ—¥å¿—]({self.admin_url}/#/logs/{project_name}/{spider.name}/{jobid})
"""
            data = {
                "msgtype": "markdown",
                "markdown": {"content": md_content},
                "mentioned_list": ["@all"],
            }
            logger.info(f"çˆ¬è™«{spider.name}æœ‰å¼‚å¸¸çŠ¶æ€ï¼Œå·²å‘é€åˆ°ä¼ä¸šå¾®ä¿¡")
            async with ClientSession() as session:
                async with session.post(self.bot_url, json=data) as response:
                    if response.status == 200:
                        result = await response.json()
                        if result.get("errcode") == 0:
                            return True
        else:
            logger.info(f"çˆ¬è™«{spider.name}æ­£å¸¸ç»“æŸï¼Œæ— éœ€å‘é€åˆ°ä¼ä¸šå¾®ä¿¡")
